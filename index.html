<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Aditya Devarakonda</title>
    <link rel="stylesheet" href="css/bootstrap.min.css">
  </head>
  <body class="webpage">

      <div class = "container">

            <div class="media-body">
              <img class="photo-object" src="img/photo.jpg"><br/>
                  <h2 align=left>Aditya Devarakonda</h2>
                  <p align=left>Assistant Research Scientist<br/>
                    <link rel="stylesheet" href="css/style.css">
                    The Institute for Data Intensive Engineering and Science<br/>
                    Johns Hopkins University</br>
                    <a href="https://scholar.google.com/citations?hl=en&user=Ao-FJHwAAAAJ">
                      Google Scholar
                    </a>|
                      <a href="https://linkedin.com/in/adevarakonda">
                      LinkedIn
                    </a>
                  </p>
                </div>

                <div class="row">
                <div class="col-md-6">
                  <h2 align=left>Contact Information</h2>
                  <p align=left>adi __AT__ jhu __DOT__ edu<br/>
                    527 Bloomberg Center<br/>
                    3400 N. Charles Street<br/>
                    Baltimore, MD 21218
                  </p>
                  <h2 align=left>Education</h2>
                  <p align=left> PhD in Computer Science, University of California, Berkeley, 2018</br>
                    MS in Computer Science, University of California, Berkeley, 2016</br>
                    BS in Computer Engineering, Rutgers University, New Brunswick, 2012
                  </p>
              </div>
        <div class = "col-md-6">
          <h2>Research Interests</h2>
          <p>High performance computing</br>
            Parallel algorithms</br>
            Algorithms complexity</br>
            Convex optimization</br>
            Numerical linear algebra</br>
            Applications of machine learning</br>
          </p>
        </div>
      </div>
        <div class="widelist">
          <h2> PhD Thesis</h2>
           A. Devarakonda, Avoiding Communication in First-Order Methods for Optimization, University of California, Berkeley, EECS, 2018. [<a href="https://escholarship.org/uc/item/3rm6d2mf">Link</a>] [<a href="bibtex/thesis.txt">bibtex</a>]
           <h2>Publications</h2>
          <ul>
            <li>A. Devarakonda, K. Fountoulakis, J. Demmel, and M. W. Mahoney, Avoiding communication in primal and dual block coordinate descent methods, SIAM J. Sci. Comp., 41(1), pp. C1-C27, 2019. [<a href="https://epubs.siam.org/doi/abs/10.1137/17M1134433">Link</a>] [<a href="bibtex/DFDM_SISC_19.txt">bibtex</a>]</li>
            <li>S. Soori, A. Devarakonda, Z. Blanco, J. Demmel, M. Gurbuzbalaban, and M. M. Dehnavi, Reducing Communication in Proximal Newton Methods for Sparse Least Squares Problems, ICPP, 2018. [<a href="https://dl.acm.org/citation.cfm?id=3225131">Link</a>] [<a href="bibtex/SDBDGD_ICPP_18.txt">bibtex</a>]</li>
            <li>A. Devarakonda, K. Fountoulakis, J. Demmel, and M. W. Mahoney, Avoiding Synchronization  in First-Order Methods for Sparse Convex Optimization, IPDPS, 2018. [<a href="https://ieeexplore.ieee.org/abstract/document/8425195">Link</a>] [<a href="bibtex/DFDM_IPDPS_18.txt">bibtex</a>]</li>
            <li>A. Devarakonda, K. Fountoulakis, J. Demmel, and M. W. Mahoney, Avoiding Synchronization  in First-Order Methods for Sparse Convex Optimization, arXiv:1712.06047, 2017. [<a href="https://arxiv.org/abs/1712.06047">Link</a>] [<a href="bibtex/DFDM_IPDPS_18.txt">bibtex</a>]</li>
            <li>A. Devarakonda, M. Naumov, and M. Garland, AdaBatch: adaptive batch sizes for training deep neural networks,  arXiv:1712.02029, 2017. [<a href="https://arxiv.org/abs/1712.02029">Link</a>] [<a href="bibtex/DNG_TR_17.txt">bibtex</a>]</li>
            <li>S. Soori, A. Devarakonda, J. Demmel, M. Gurbuzbalaban, and M. M. Dehnavi, Avoiding communication in proximal methods for convex optimization problems, arXiv:1710.08883, 2017. [<a href="https://arxiv.org/abs/1710.08883">Link</a>] [<a href="bibtex/SDDGD_TR_17.txt">bibtex</a>]</li>
            <li>A. Gittens, A. Devarakonda, E. Racah, M. Ringenburg, L. Gerhardt, J. Kottalam, J. Liu, K. Maschhoff, S. Canon, J. Chhugani, P. Sharma, J. Yang, J. Demmel, J. Harrell, V. Krishnamurthy, and M. W. Mahoney, Matrix factorizations at scale: A comparison of scientific data analytics in Spark and C+ MPI using three case studies, IEEE Big Data, 2016. [<a href="https://ieeexplore.ieee.org/abstract/document/7840606">Link</a>] [<a href="bibtex/GD_BD_16.txt">bibtex</a>]</li>
            <li>A. Devarakonda, K. Fountoulakis, J. Demmel, and M. W. Mahoney, Avoiding communication in primal and dual block coordinate descent methods, arXiv:1612.04003, 2016. [<a href="https://arxiv.org/abs/1612.04003">Link</a>] [<a href="bibtex/DFDM_arxiv_16.txt">bibtex</a>]</li>
            <li>A. Gittens, A. Devarakonda, E. Racah, M. Ringenburg, L. Gerhardt, J. Kottalam, J. Liu, K. Maschhoff, S. Canon, J. Chhugani, P. Sharma, J. Yang, J. Demmel, J. Harrell, V. Krishnamurthy, M. W. Mahoney, and Prabhat, Matrix Factorization at Scale: a Comparison of Scientific Data Analytics in Spark and C+MPI Using Three Case Studies , arXiv:1607.01335, 2016. [<a href="https://arxiv.org/abs/1607.01335">Link</a>] [<a href="bibtex/GD_arxiv_16.txt">bibtex</a>]</li>
            <li>R. Carbunescu, A. Devarakonda, J. Demmel, S. Gordon, J. Alameda, and S. Mehringer, Architecting an autograder for parallel code, XSEDE, 2014. [<a href="https://dl.acm.org/citation.cfm?id=2616571">Link</a>] [<a href="bibtex/CDDGAM_XSEDE_14.txt">bibtex</a>]</li>
            <li>M Parashar, M AbdelBaky, I Rodero, and A Devarakonda, Cloud paradigms and practices for computational and data-enabled science and engineering, Comp. in Sci. Eng., 15(4), pp. 10-18, 2013.  [<a href="https://ieeexplore.ieee.org/abstract/document/6530588">Link</a>] [<a href="bibtex/PARD_CISE_13.txt">bibtex</a>]</li>
            <li>D. Villegas, N. Bobroff, I. Rodero, J. Delgado, Y. Liu, A. Devarakonda, L. Fong, S. M. Sadjadi, and M. Parashar, Cloud federation in a layered service model, J. Comp. and Sys. Sci., 78(5), pp. 1330-1344, 2012.  [<a href="https://www.sciencedirect.com/science/article/pii/S0022000011001620">Link</a>] [<a href="bibtex/VB_JCSS_12.txt">bibtex</a>]</li>
          </ul>
          <h2>Talks</h2>
          <ul>
            <li>"Unsupervised Learning", Guest Lecture, AS.171.205 Beautiful Data: Introduction to Practical Data Science, Johns Hopkins University, April 29th, 2020.</li>
            <li>"An Introduction to Deep Learning", Guest Lecture, AS.171.205 Beautiful Data: Introduction to Practical Data Science, Johns Hopkins University, April 27th, 2020.</li>
            <li>"Unsupervised Learning", Guest Lecture, AS.171.205 Beautiful Data: Introduction to Practical Data Science, Johns Hopkins University, April 29th, 2019.</li>
            <li>"An Introduction to Deep Learning", Guest Lecture, AS.171.205 Beautiful Data: Introduction to Practical Data Science, Johns Hopkins University, April 24th, 2019.</li>
            <li>"Avoiding Communication in Machine Learning", CS Seminar Lawrence Berkeley National Lab, Berkeley, CA, January 25th, 2019.</li>
            <li>"Avoiding Synchronization in First-Order Methods for Sparse Convex Optimization", IPDPS, Vancouver, Canada, May, 2018.</li>
            <li>"Avoiding Communication in First-Order Methods for Optimization", Dissertation Talk, Berkeley, CA, May 2018.</li>
            <li>"s-Step Methods in Machine Learning", SIAM Parallel Processing, Tokyo, Japan, March 8th, 2018.</li>
            <li>"Avoiding Synchronization in Sparse Convex Optimization", Scientific Computing and Matrix Computation Seminar, Berkeley, CA, February, 2018.</li>
            <li>"Avoiding Communication in Machine Learning", Johns Hopkins University Applied Physics Lab Seminar, Laurel, MD, January, 2018.</li>
            <li>"Communication-Avoiding Krylov Subspace Methods", NASA Langley Research Center, Hampton, VA, December 2017.</li>
            <li>"Communication-Avoiding Algorithms", NASA Langley Research Center, Hampton, VA, December 2017.</li>
            <li>"AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks", NVIDIA, Santa Clara, CA, November, 2017.</li>
            <li>"Communication-Avoiding Methods for Regularized Least Squares", SIAM Annual Meeting, Pittsburgh, PA, July 2017.</li>
            <li>"Communication-Avoiding Machine Learning", Householder XX Symposium, Blacksburg, VA, June 2017.</li>
            <li>"Communciation-Avoiding Machine Learning", ASPIRE Summer Retreat, Santa Cruz, CA, June 2017.</li>
            <li>"Communication-Avoiding Primal and Dual Block Coordinate Descent Methods", SIAM Optimization, Vancouver, Canada, May 2017.</li>
            <li>"A Generalized Framework for Communication-Avoiding Regularized Least Squares", Berkeley Statistics Annual Research Symposium, Berkeley, CA, April 2017.</li>
            <li>"Communication-Avoiding Machine Learning: Block Coordinate Descent", Guest Lecture, CS267: Applications of Parallel Computers, University of California, Berkeley, March, 2017.</li>
            <li>"Matrix factorization at scale: A comparison of scientific data analytics in Spark and C+ MPI using three case studies", IEEE Big Data, Washington DC, December 2016.</li>
            <li>"Avoiding Communication in Machine Learning", Berkeley Statistics Annual Research Symposium, Berkeley, CA, April 2016.</li>
            <li>"Communication-Avoiding Coordinate Descent Methods for Linear Systems", Scientific Computing and Matrix Computation Seminar, Berkeley, CA, November, 2016.</li>
          </ul>
        </div>
        <div class="copyright">
          <p align=center>
            &copy; Aditya Devarakonda. All rights reserved.
          </p>
        </div>
      </div> <!-- end container -->
  </body>
</html>
