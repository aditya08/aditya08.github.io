---
title: ""
date: 2021-08-09
draft: false
---

### Communication-Avoiding Machine Learning
<figure>
<img src=/images/caml_fig.gif width="400">
<!--figcaption>Gram Matrix Partitioning for CA-ML.</figcaption-->
</figure>

Machine learning has gained renewed interest in recent years due to advances in computer hardware (processing power and high-bandwidth storage) and the availability of large amounts of data which can be used to develop accurate, robust models. While hardware improvements have facilitated the development of machine learning models in a single machine, the analysis of large amounts of data still requires parallel computers to obtain shorter running times.

This project focuses on developing novel techniques and algorithms to reduce the communication bottleneck when performing machine learning tasks at scale on parallel machines. Topics in this area include: i) redesigning existing machine learning algorithms to reduce communication, ii) studying the asymptotic and numerical behavior of these algorithms, and iii) implementing these algorithms as part of a high performance software library.
<!--
### Scalable Deep Learning
<figure>
<img src=/images/dl_fig.gif width="400">
<!--figcaption>Neural Network Forward Pass.</figcaption>
</figure>
-->