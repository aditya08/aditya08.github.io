<!doctype html><html lang=en data-theme><head><title>Aditya Devarakonda</title><meta charset=utf-8><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=description content="Assistant Professor at Wake Forest University

aditya 'at' wfu.edu
 251 Manchester Hall"><link rel=stylesheet href=../css/style.min.d242d82808dd715e9d5961c4cf8584987c1710d14979ef8a39e9ced0045b7782.css integrity="sha256-0kLYKAjdcV6dWWHEz4WEmHwXENFJee+KOenO0ARbd4I=" crossorigin=anonymous type=text/css><link rel=stylesheet href=../css/markupHighlight.min.f798cbda9aaa38f89eb38be6414bd082cfd71a6780375cbf67b6d2fb2b96491e.css integrity="sha256-95jL2pqqOPies4vmQUvQgs/XGmeAN1y/Z7bS+yuWSR4=" crossorigin=anonymous type=text/css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css><link rel="shortcut icon" href=../favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=../favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=../favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=../favicons/favicon-16x16.png><link rel=canonical href=../research/><script type=text/javascript src=../js/anatole-header.min.44d2f120151978580303a6720d28c5d346066534fcb0c8ad76697cc49e5612b4.js integrity="sha256-RNLxIBUZeFgDA6ZyDSjF00YGZTT8sMitdml8xJ5WErQ=" crossorigin=anonymous></script>
<script type=text/javascript src=../js/anatole-theme-switcher.min.e18c288b42b3e223e5c95c4c07e47fe6c431c1c503ce1deb1e3d1c658304b313.js integrity="sha256-4Ywoi0Kz4iPlyVxMB+R/5sQxwcUDzh3rHj0cZYMEsxM=" crossorigin=anonymous></script><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://aditya08.github.io/images/site-feature-image.png"><meta name=twitter:title content><meta name=twitter:description content="Communication-Avoiding Machine Learning  Gram Matrix Partitioning for CA-ML.  Machine learning has gained renewed interest in recent years due to advances in computer hardware (processing power and high-bandwidth storage) and the availability of large amounts of data which can be used to develop accurate, robust models."></head><body><div class="sidebar ."><div class=logo-title><div class=title><img src=https://aditya08.github.io/images/profile.jpg alt="profile picture"><h3 title><a href=../>Aditya Devarakonda</a></h3><div class=description><p>Assistant Professor at Wake Forest University<br><br>aditya 'at' wfu.edu<br>251 Manchester Hall</p></div></div></div><ul class=social-links><li><a href=https://github.com/aditya08/ rel=me aria-label=GitHub target=_blank><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li><a href="https://scholar.google.com/citations?user=Ao-FJHwAAAAJ&hl=en" rel=me aria-label=GoogleScholar target=_blank><i class="ai ai-google-scholar fa-2x" aria-hidden=true></i></a></li><li><a href=../data/aditya_cv.pdf rel=me aria-label=CV target=_blank><i class="ai ai-cv fa-2x" aria-hidden=true></i></a></li><li><a href=https://orcid.org/0000-0002-8251-9150 rel=me aria-label=ORCID target=_blank><i class="ai ai-orcid fa-2x" aria-hidden=true></i></a></li></ul><div class=footer><div class=by_farbox>&copy; Aditya Devarakonda 2021</div></div></div><div class=main><div class="page-top ."><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true></span>
<span aria-hidden=true></span>
<span aria-hidden=true></span></a><ul class=nav id=navMenu><li><a href=../ title>Home</a></li><li><a href=../about/ title>About</a></li><li><a href=../teaching/ title>Teaching</a></li><li><a class=current href=../research/ title>Research</a></li><li><a href=../publications/ title>Publications</a></li><li class=theme-switch-item><a class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></li></ul></div><div class=autopagerize_page_element><div class=content><div class="post ."><div class=post-content><div class=post-title><h3></h3></div><h3 id=communication-avoiding-machine-learning>Communication-Avoiding Machine Learning</h3><figure><img src=../images/caml_fig.gif width=400></figure><p>Machine learning has gained renewed interest in recent years due to advances in computer hardware (processing power and high-bandwidth storage) and the availability of large amounts of data which can be used to develop accurate, robust models. While hardware improvements have facilitated the development of machine learning models on a single machine, the analysis of large amounts of data still requires parallel computers to obtain shorter running times.</p><p>This project focuses on developing novel techniques and algorithms to reduce the communication bottleneck when performing machine learning tasks at scale on parallel machines. Topics in this area include: i) redesigning existing machine learning algorithms to reduce communication, ii) studying the asymptotic and numerical behavior of these algorithms, and iii) implementing these algorithms as part of a high performance software library.</p><h3 id=scalable-deep-learning>Scalable Deep Learning</h3><figure><img src=../images/dl_fig.gif width=400></figure><p>Deep learning is the enabling technology behind many applications that we use daily, from FaceID (Image Recognition) and Alexa (Voice Recognition) to Autopilot (Self-Driving Cars). All of these technologies (and many more) utilize deep learning models under the hood to solve complex optimization problems with high accuracy, in some cases, superhuman accuracy.</p><p>Large deep learning models are trained on multi-node, multi-GPU machines and require several days of training. This project focuses on developing new training techniques, optimization problems, and computational kernels to improve performance. Topics in this area include: i) implementing new (quasi) second-order optimization methods, ii) redesigning optimization methods to reduce communication, and iii) implementing new matrix and tensor factorizations to compress and speedup training.</p></div><div class=post-footer><div class=info></div></div></div><div>Last Modified: Friday, May 6, 2022.<br><br><br></div></div></div></div><script type=text/javascript src=../js/medium-zoom.min.f38354b14e726daea2b656ccc32c944fa1e10a974998cb85bbb50c0f836878b4.js integrity="sha256-84NUsU5yba6itlbMwyyUT6HhCpdJmMuFu7UMD4NoeLQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css integrity=sha384-t5CR+zwDAROtph0PXGte6ia8heboACF9R5l/DiY+WZ3P2lxNgvJkQk5n7GPvLMYw crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js integrity=sha384-FaFLTlohFghEIZkw6VGwmf9ISTubWAVYW8tG8+w2LAIftJEULZABrF9PPFv+tVkH crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js integrity=sha384-bHBqxz8fokvgoJ/sc17HODNxa42TlaEhB+w8ZJXTc2nZf1VgEaFZeZvT4Mznfz0v crossorigin=anonymous onload=renderMathInElement(document.body)></script></body></html>